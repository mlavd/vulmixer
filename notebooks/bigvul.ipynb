{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba1e2d-1b29-4187-9d4d-0902ae870b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285b42d-0eae-432c-850a-68d6ae36f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for model in Path('/data/fire/logs/bigvul/').iterdir():\n",
    "    # print(model.stem)\n",
    "    preds = []\n",
    "    truth = []\n",
    "\n",
    "    for dataset in [ 'codexglue', 'd2a', 'draper' ]:\n",
    "        # print('\\t', dataset)\n",
    "        df_safe = pd.read_csv(model.joinpath(f'{dataset}-safe.csv'))\n",
    "        df_vuln = pd.read_csv(model.joinpath(f'{dataset}-vuln.csv'))\n",
    "        \n",
    "        ds_preds = np.concatenate([df_vuln.before, df_vuln.after, df_safe.before])\n",
    "        ds_truth = np.zeros(ds_preds.shape)\n",
    "        ds_truth[:df_vuln.shape[0]] = 1\n",
    "        \n",
    "        assert ds_preds.shape[0] == (df_safe.shape[0] + 2 * df_vuln.shape[0]), 'Preds is the wrong shape'\n",
    "        assert ds_truth.shape == ds_preds.shape, 'Truth is the wrong shape'\n",
    "        assert (ds_truth == 0).sum() == df_safe.shape[0] + df_vuln.shape[0], 'Wrong number of safe values'\n",
    "        assert (ds_truth == 1).sum() == df_vuln.shape[0], 'Wrong number of vuln values'\n",
    "        \n",
    "        preds.append(ds_preds)\n",
    "        truth.append(ds_truth)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    truth = np.concatenate(truth)\n",
    "\n",
    "    data.append({\n",
    "        'model': model.stem,\n",
    "        'f1': f1_score(truth, preds > 0.5),\n",
    "        'precision': precision_score(truth, preds > 0.5, zero_division=0),\n",
    "        'recall': recall_score(truth, preds > 0.5),\n",
    "    })\n",
    "\n",
    "preds = np.zeros(preds.shape)\n",
    "data.append({\n",
    "    'model': 'safe baseline',\n",
    "    'f1': f1_score(truth, preds),\n",
    "    'precision': precision_score(truth, preds, zero_division=0),\n",
    "    'recall': recall_score(truth, preds),\n",
    "})\n",
    "\n",
    "preds = np.ones(preds.shape)\n",
    "data.append({\n",
    "    'model': 'vuln baseline',\n",
    "    'f1': f1_score(truth, preds),\n",
    "    'precision': precision_score(truth, preds, zero_division=0),\n",
    "    'recall': recall_score(truth, preds),\n",
    "})\n",
    "        \n",
    "data = pd.DataFrame(data)\n",
    "data = data.sort_values(by='f1', ascending=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef8f3e-2d3f-496a-8e95-817d3165c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\\\n",
    "    .style.format({\n",
    "        'f1': '{:.4f}',\n",
    "        'precision': '{:.4f}',\n",
    "        'recall': '{:.4f}',\n",
    "    }).background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e7b17-8001-4e15-b513-74cf560c20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vuln = 100\n",
    "\n",
    "df = data.copy()\n",
    "df['fp'] = (1 / df.precision - 1) * vuln\n",
    "df['fn'] = vuln - df.recall * vuln\n",
    "df[['model', 'fp', 'fn']].round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
